Q60.
NLP用のライブラリであるgensim（https://radimrehurek.com/gensim/models/keyedvectors.html）を使う。
謎のバイナリファイルになってるがKeyedVectorsを使ってロードすれば多分keyが単語でvalueがベクトルになった辞書型になってくれる。


Q62.
便利なライブラリがいっぱいある。
most_similar(positive=["United_States"])とすれば"United_States"に近い上位10単語が出てくる。most_similar_cosmul("United_States")でも良い。


Q63.
ベクトルの足し引きもmost_similar(positive=["Spain", "Athens"], negative = ["Madrid"]))みたいな感じでできる。すごい


Q66.
Scipy使えばよかった......。
スピアマンの相関係数は、ある二つの順序の近さを評価する指標。
Dを対応するペアの順位差、Nをペア数として、1 - \frac{6 \sum D^2}{N^3 - N}として表される。
これが大きいほど順番が似ている。


Q67.
これもsklearnにあったらしい。
最初はランダムにカテゴリを割り振って、そのカテゴリの重心を計算→重心が一番近いカテゴリにもう一回割り振って同じことをして、カテゴリの集合が変化しなければ終わり、みたいな。


Q68.
「階層クラスター分析」というのにWard法（ウォード法）は用いられる。
階層クラスター分析は、もっとも似ている（近い）組み合わせから順番にまとめてクラスターを作る手法。
途中の過程が階層のように表せて、最終的にはトーナメント表みたいな樹形図（デンドログラム）で可視化される。
途中でクラスター同士の距離を比較する必要が出てくるが、ウォード法はそういったクラスター間の距離を測定する手法の一つ。（他にも群平均法や最短距離法などがある）
L(C)を、クラスタCの重心からC内の各点に対する距離の二乗和（つまりどれだけ点が散らばっているか）とすると、クラスタC_1とC_2のウォード法における距離は、L(C_1 \cup C_2) - L(C_1) - L(C_2)となる。合体してもばらつきが増えない（ウォード法で計算される距離が小さい）クラスタ同士が合体される。


Q69.
T-SNEは次元を削減するアルゴリズムで、今回は高次元（300次元）の単語のベクトル表現を2次元に落としてプロットしてみよう、みたいな趣旨。
細かいところは全く追えてないのでアレだけど、地理的に近い国のベクトルは大体似たようなところにプロットされてるな〜というのを眺めて終了。
これもsklearnにあったのでそれをそのまま利用。こんなんでいいのか……。
（詳しい説明→ https://qiita.com/g-k/items/120f1cf85ff2ceae4aba）




