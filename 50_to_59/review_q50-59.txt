Q51. 特徴量って何やねん(Wikipedia)

＊＊＊＊＊＊＊＊＊＊＊＊＊＊＊＊＊＊＊＊＊＊＊＊＊＊＊＊＊＊＊＊＊
特徴量はデータを変形して得られ、その特徴を表現し、続く処理に利用される数値である。
特徴量はデータを変換することで生成される。この変換を特徴抽出という。専門家の知見を用いた人手による変換規則の探求は特徴量エンジニアリングと呼ばれ、機械学習による場合は表現学習（BERTとか主成分分析とか）と呼ばれる。
特徴抽出は観測値/生データを特徴量空間へと射影するというニュアンスから「埋め込み（英: embedding）」とも呼ばれる。自然言語処理では単語に対する特徴抽出が「単語の埋め込み（word embedding）」と呼ばれる。
＊＊＊＊＊＊＊＊＊＊＊＊＊＊＊＊＊＊＊＊＊＊＊＊＊＊＊＊＊＊＊＊＊

要は生データを機械が扱える形に変えたもの。今回はテキストをベクトル表現にするので、そのベクトルが特徴量となる。
ベクトルを作るのが特徴抽出になるので、これはtf-idfを使う。（Sklearnにtf-idf Vectorizerがある）

Numpy配列を保存・ロードするには、numpyのsavetxtやloadtxtを使うとうまくいく。


Q52.
ロジスティック回帰分析は、あるデータがどのカテゴリに属するかを判定してくれる。例えば喫煙の有無や運動時間、食事などのデータから、その人が将来的に病気になるか否かを判定するみたいな感じ。入力となるデータは複数あってもよいし、判定されるカテゴリも2個だけでなくそれ以上あっても良いが、結局は複数のデータをもらってそれをどのカテゴリに分類するか？みたいなことになる。今回はQ51で求めた文章の素性のベクトルをもとに、4種類のニュースのカテゴリに分類する。

SklearnのLogisticRegressionに投げる。カテゴリ（目的変数）が2個（例えば0と1みたいな）のロジスティック回帰モデルは、説明変数\vec{x} = (x_1, ..., x_n)に対して重み（回帰変数）\vec{b} = (b_1, ..., b_n)がある時、1と分類される確率が　p_1(x) = 1/(1 + e^{\vec{x} \cdot \vec{b}})　と表される。

ここではカテゴリが4つあるので、これは多項ロジスティック回帰モデルという。カテゴリcと予測される確率は、先ほど同様に、カテゴリ0用の重み\vec{b}_c = (b_{c1}, ..., b_{cn})を用いてp_c(x) = 1/(1 + e^{\vec{x} \cdot \vec{b}_c})みたいな感じになる。

このモデルは説明変数から質的データ（不連続、値の足し引きに意味がないデータ）を予測するのに使う。普通の線形回帰モデルは量的データ（連続的なデータ）を予測するために最小二乗法とかでなるべく誤差を減らすように近似するのに対し、こちらは予測の精度を上げるのが目的なので最尤推定や交差エントロピーなどを用いる。

カテゴリが2個だと、ある入力xに対する出力y \in \{0, 1\}の交差エントロピーは、ロジスティック関数で出力が1と予測される確率p(x)を用いて、
-y \log p(x) - (1 - y) \log (1 - p(x)) となる。（例えば出力が0と予測されてほしいケースなら、- \log (1 - p(x))が残り、これは1と予測される確率が0なら最小の0を取り、そうでない場合はどんどん大きくなる。これを最小化するのが目標で、そうなれば全て正しく予測できることになる）

交差エントロピーを用いた損失関数は、全部のサンプル（入力）について交差エントロピーをとってその平均を取る。それを各重みで微分して最急降下法なりを使って、重みを次第に更新していく。

予測するべきカテゴリが複数ある場合は、対数尤度（パラメータがこれこれの時に、入力xについて出力yとなる確率がこうですよ、という尤度について対数をとったもの）を最大にするようなパラメータを最尤推定法で決定する。（詳細はhttps://www.ier.hit-u.ac.jp/~kitamura/lecture/Hit/08Statsys8.pdf）

学習したモデルはpickleで保存できる。（https://localab.jp/blog/save-and-load-machine-learning-models-in-python-with-scikit-learn/）



Q55. 混同行列（confusion matrix）って何？
クラス分類問題の結果をまとめた表。例えばポジネガ分類であれば、

                 Predicted
                 Negative  Positive
Actual Negative     TN        FP
       Positive     FN        TP

という表ができる。今回はカテゴリが4つあるので、4*4の行列ができる。行列の対角要素がカテゴリを正しく予測できた場合に相当し、各行が各カテゴリに実際に属するデータがどのように分類されたかを示している。




Q56. 
・適合率　→　カテゴリAと予測したものの中で実際にAに属しているものの割合（真陽性を真陽性＋偽陽性で割ったもの）。
・再現率　→　実際にカテゴリAに入っているものの中で正しくAと予測できたものの割合（真陽性を真陽性＋偽陰性で割ったもの）
・F1スコア　→　適合率と再現率の調和平均（分子同士、分母同士をそれぞれ足してもう一回割る）
・マイクロ平均　→　全データの真陽性や偽陰性などをまとめて適合率などを計算する。例えば適合率のマイクロ平均は、全データ数で真陽性であるケースの数を割ったもの、という感じ。
・マクロ平均　→　適合率や再現率などを全ケースに渡って平均すると出る。




Q57.
Tf-idfの計算で得たワード順にLogisticRegressionのcoef_(係数行列)の要素が並んでいる（これの各行が各カテゴリの確率を出すときの係数に相当している。4行n列（nは単語数）の係数行列bについて、カテゴリcに分類される確率が 1/(1+exp^(-(x_1 b_{c1} + x_2 b_{c2} + … + x_n b_{cn}))みたいな）ので、各カテゴリごとに絶対値が大きい係数を出す。
Argsortをすると、ソートした結果がインデックスで返ってきて便利。



Q58.
L1正則化は最小化したい目的関数にパラメータ（ベクトルの各要素）の絶対値を足したもの、L2正則化は各要素の2乗を足したものを目的関数とする。
L1正則化は不要な説明変数を0にしてモデルを単純化することを目的にしており、L2正則化はパラメータ自体が大きくなるのを防いで特定の説明変数が独占的な表現を持たないように（過学習を防ぐように）することを目的としている。SklearnのデフォルトはL2正則化。
SklearnのLogisticRegressionでは、正則化パラメータとしてCが指定されており、この大小でどれくらい正則化をキツくするか（目的関数にプラスする変数の大きさの部分の影響力を大きくするか、で合ってるかしら）を設定できる。Cのデフォルトは1で、これを小さくすると正則化がキツくなり、大きくすると緩くなる。




