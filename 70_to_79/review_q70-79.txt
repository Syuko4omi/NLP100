NNについて具体的な実装法→https://qiita.com/ya-ya-kaz716/items/b4a3280e292a8c4373db

70~73
・勾配降下法：「訓練データ全てを使って」勾配を計算し、重みから（勾配の総和）*（学習率）を引く→バッチ学習
・確率的勾配降下法（Stochastic Gradient Descent, SGD）：「一つもしくは複数の訓練データ（ミニバッチ）を用いて」勾配を計算し、重みから（勾配の総和）*（学習率）を引く

・分類問題では交差エントロピーが使われ、それは正解カテゴリiに分類される確率p(i)を使って-log p(i)で計算される。（p(i)が1→100%正しく予測できるなら最小の0、p(i)が0→全然的外れならメチャクチャデカくなる）
・別に誤差関数は何を使ってもうまくいくかもしれないが、例えば二乗誤差とかと比べた時に学習が早くなるかもしれないということがある。
・勾配は損失関数の微分で得られるので、二乗誤差は一次関数になる一方でクロスエントロピーは1/p(i)なので、p(i)が正解と離れているとそれが大きく反映される。

[注意]Tensor型に変換するときにテンソルの中の数値がtorch.double型になってしまうので、明示的にfloatに変換する。

・NNにおける勾配の話→https://www.f.waseda.jp/yusukekondo/TALLFALL19/TALLFALL0402.html



78, 79
GPUを使って学習をするために、これまでのコードにいくつか追加する部分がある。

・device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')
・net = (NNのクラス).to(device)

[注意]計算に必要な要素はCPUとGPUの別々の場所に置かれてあるとエラーが出るので、最初に
・torch.set_default_tensor_type('torch.cuda.FloatTensor')を宣言しておくと、テンソルを作成する際のデバイスをCPUからGPUに変更することができる（ので入れる）。


