Q91
PyTorchで頑張って実装。Colabに課金してColab Proを使って実験。

全体の流れとしては、
(i)：英文と日本語文のテキストから、3回以上登場する単語のみピックアップする（q91_create_removed_word_list.py）
(ii)：(i)で作った単語のリストを参考に、それぞれの言語に関する語彙の情報（単語とidの一対一対応など）を作成（q91_create_vocab_data.py）
(iii)：実際にモデルを訓練

という形になる。モデルにはLSTMを用いたエンコーダー・デコーダーモデルを採用した。

★ハマった場所：デコーダー(OneStepDecoder)の出力にsoftmaxをかけて計算し、その結果をそのままCrossEntropyLossに流してしまった。
CrossEntropyLossの中でもソフトマックスの処理が行われるので、2回かけてしまうことでメチャクチャになってしまった。
学習の時は全結合層の結果をそのままCrossEntropyLossに受け渡し、推論の時は自分で改めてソフトマックスをかけるようにするべき。（https://qiita.com/ground0state/items/8933f9ef54d6cd005a69）

あと、エンコーダーの最後の出力をそのまま文脈ベクトルとして用いているが、本来はPaddingの部分よりも前の時間ステップでEOSトークンを読んだ後の層を文脈ベクトルとして使うべきだと思う。
アテンションではちゃんとマスクをして計算しているのにこれだとちょっとどうなのよ、感が否めない。→98でAllenNLPのを流用して解決します。

バッチサイズは128、teacher forcing ratioは訓練のエポック数が全体の1/3以内であれば常に1.0、それ以降は0.9として学習。
Q97あたりで後述するが、teacher forcing ratioをもっと下げるべきだった気がする。



Q92
実際に翻訳する。OOVが予測として出てきたらそのままOOVと出力する。



Q93
nltkのBLEUスコアで評価。
何もしないと4-gram以下の一致がないと即座にBLEUスコアが0になるので、smoothing functionを噛ませてあげる。
ただデフォルトの4-gram以下で評価を使うと、例えば2単語の例（京都府→Kyoto prefecture）がちゃんと訳せていたとしてもBLEUスコアが0.3とかになっちゃってキツい。
どうするか悩んだけどとりあえず4-gram以下にして、BLEUスコア0.087を得た。2-gram以下だと0.18くらいになる。



Q94
ビームサーチをする。
ビームサイズを大きくするとBLEUスコアが微妙に向上することを確認。



Q95
サブワード化をする。
Q91~94では、データを作る際に形態素解析をしていたが、語彙のサイズが大きくなってしまうという問題があった（英単語もスペースで区切った単語をそのまま使うと16万次元とかになってしまう。Q91では低頻度語を除いたが、それでも5万次元とかあった）。
それを避けるため、低頻度語を文字や部分文字列に分解する。これをサブワード化と呼ぶ。

サブワード化をするためにSentencePiece（https://github.com/google/sentencepiece）というトークナイザを使う。
Q91~94では最初から形態素解析をしたデータ（cln）を使っていたが、SentencePieceに文章のデータを丸投げすると、こちらが指定した適当な語彙数にまでサブワード化をしてくれる（モデルを作ってくれる）。

作ったモデルを使えば、文章のトークナイズや、逆にidから復元もできる。
Pythonでの使い方→https://github.com/google/sentencepiece/blob/master/python/README.md


訓練データには、元のデータ中にあるものの中で、日文と英文それぞれidにエンコードした時に長さが50以下になるもののみ採用した。




Q96
参考にしたページ
https://qiita.com/nj_ryoo0/items/f3aac1c0e92b3295c101
https://blacktanktop.hatenablog.com/entry/2020/07/18/090754

Colabでのやりかた
①なんか学習を走らせる。
その際にfrom torch.utils.tensorboard import SummaryWriterのおまじないをする。

②記録したいデータを記録する。
writer = SummaryWriter(log_dir="./logs/hoge")を宣言して、ログのファイルの出力先を指定。
それから、writer.add_scalar('ラベル名', 値, タイムステップ)として逐次記録する。

③データを表示する。
記録が終わったら
%load_ext tensorboard
%tensorboard --logdir=./logs
で表示させる。




Q97
Teacher Forcing Ratio(TFR)のみ変更してどうなるか見てみる。
普通に驚いたのだが、TFR = 1.0（常に教師信号を使って学習する）よりも、適切に自分の出力を次の入力に使う方が（TFR < 1.0の方が）検証データだけでなく訓練データでもlossが小さくなった。

スクショにある通り、6エポックのうち最初の2エポックをtfr=1.0、それ以外をtfr = 1.0, 0.9, 0.8でやったが、tfrが小さくなるほどlossが減るという感じだし、tfr=0.7を一回だけやった時と上のtfr=0.8がほぼ変わらないので、tfr = 0.7を1エポックやれば十分という感じ。

学習時のロスはtfr=1.0の方が減ってるように見えるんだけど、それはあくまで答えが与えられている上でのロスの減り方なので、実際の状況(tfr=0)においてロスが減るもので学習すべきだったということだろうか。
かといってtfr=0.7を1回回した時とtfr=1.0を5回回した時の翻訳を比較するとtfr=1.0の方が普通にマシな翻訳をしているのでどうして……みたいな気持ちになる。

最終的にはtfr=1.0で5エポック、tfr=0.7で10エポック回してモデルを学習することにした。
BLEUスコア(nltkのライブラリ、BLEU-4)だと平均して0.103くらい。4-gram完全一致がないといくら一致していても（例えば4単語に満たない訳文は）BLEUスコアが1.0を下回るので、weightsを調整して2-gram完全一致にすると（BLEU-2にすると）BLEUスコアは0.21くらいになった。




Q98
ドメイン適応の手法には、Multi-domain学習（ドメイン内データとドメイン外データを混ぜる）とFine-tuningがあるので、後者を採用。

JESCはこんな感じ。
i i wonder...	捕まえたよ。 ほーら。 ウホホホホホホ...。
no need.	黒子ひょっとしていらない子


...なので、JParaCrawlを採用。JParaCrawlもダブりがあったりガチ機械翻訳だったりとお世辞にも綺麗とはいえないが、いずれにせよJESCよりはマシなので適当に整形したものを使う。
2500万件もデータがあって扱いきれないので、そのうち精度の高そうな19万件を拾ってくる（それぞれのデータの横にある謎の数字が0.785より大きいものを採用）。

こういうのはちゃんと全部網羅的に使うべきなのかな、とは思ったけど、ただでさえ30万件の訓練データでまごまごしているのでその100倍近くあるものは扱いきれないかなと思ったのでそのようにした。（間違っているかもしれない）

学習は、まずはkfttのデータを使ってtfr=1.0で5エポック、tfr=0.7で10エポック回してモデルを学習し、それからJParaCrawlを使って同様にtfr=1.0で5エポック、tfr=0.7で10エポック回してfine-tuneするという形にした。
結果としては散々で、性能向上どころの話ではなかった。（というか別のドメインのデータで学習しても元のドメインにおける性能が上がることはなくないか？）
ファインチューニングのやり方がそもそも間違っているのかもしれないが、とりあえず今回はもうここまでにしておく。
