Q80~85

[状況確認]
最初に実装したもの→RNNを多層にしてハイパラ設定頑張ってもtest accuracyが75%とかそんなもんで頭打ちになってしまう。
明らかにマズそうな部分として、OOVのidの0が(テンソルにする上で必要な)paddingの0と被っている点があるので、そこを解決すべきな気がする。

[問題点]
現状は、paddingの0とOOVの0はいずれもid=0なので、embeddingの時にpadding_idx=0を指定するといずれも零ベクトルとして解釈される。これがよろしくない点は
・例えばidが[1,0,2,3,0,0]みたいな状況（1の次の0はoovの0、3の次の0はpaddingの0）の時、oovの0は適当な埋め込み表現(oov用のベクトル)で置くべき
・上記のようにpaddingがあるなら、それはRNNの計算で使わないべき（零ベクトルをRNNに入れたとしても隠れ層は変化するため）
の2点を無視していること。

さらに、バッチについてpaddingを施す時に、そのpaddingを全体の最大長になるように合わせているので、必要な部分に対して不必要な部分が多すぎる点がある。

[対策]
とりあえず、対策としてできることは
・OOVのidは0ではなくvocab_size+1にする（これは問題文の指示を無視しちゃってるけど、paddingはid=0として扱う以上こうせざるをえない）
・paddingの部分を無視する（RNNに突っ込まない）


[改善の結果]
・oovのidを0にしようがvocab_size+1にしようが大して変わらない
・paddingを無視してもしなくても大して変わらない
・結局最後はハイパラ調整

★問90以降で、RNNの最後にソフトマックスをかけた出力をクロスエントロピー誤差に突っ込むとロスが全然減らないみたいな現象があり、その部分を直すとここもロスが順調に減ることを確認した。



Q86,87
https://qiita.com/icoxfog417/items/5fd55fad152231d706c2
https://qiita.com/icoxfog417/items/5aa1b3f87bb294f84bac


Q89
超参考になった→ https://www.analyticsvidhya.com/blog/2020/07/transfer-learning-for-nlp-fine-tuning-bert-for-text-classification/

転移学習（事前に学習したモデルを他に応用すること）をする。文章を埋め込み表現にするためにBERTを使う。
やることの流れとしては、
・データの整形
・モデルの作成
・学習
という感じ。

データの整形は2ステップあり、まずタイトルをtokenizeして番号の列として扱う。それからテンソル型に変換し、バッチ学習ができるようにSamplerを用いて小分けにすればOK。

モデルの作成は、これまでやってきたNNの入力を768次元の埋め込み表現にするだけ。

学習も交差エントロピー誤差で評価した損失をoptimizerにかけるだけで大丈夫。
ただfine-tuningをする過程で、データ数に偏りがあるためなのか、データ数の少ない2つのカテゴリに判定されるものが出ないという問題があったので、訓練データでの頻度を用いて重さにバイアスをかけた。
カテゴリiにかかる重さは(全部のサンプル数)/((カテゴリ数)*(カテゴリiの事例数))で計算される。
例えばラベルが10個（0が8個、1が2個）なら、カテゴリ0の重みは10/2*8=0.625と計算される。
